{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab #1 - Creating your first pipeline with Amazon SageMaker Components for Kubeflow\n",
    "\n",
    "We can start by upgrading/verifying the installation of the Kubeflow Pipelines SDK in our Jupyter environment.\n",
    "\n",
    "We will also verify that we have available the Domain Specific Language (DSL) compiler for Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp --upgrade\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import the Amazon SageMaker and Boto3 SDKs for being able to get the proper roles, and interact with other services like Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm   = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will upload to Amazon S3 the dataset that we will use for this example, together with our pre-processing script. Both files are available in our local Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp kmeans_preprocessing.py s3://$bucket_name/mnist_kmeans_example/processing_code/kmeans_preprocessing.py\n",
    "!aws s3 cp mnist.pkl.gz s3://$bucket_name/mnist_kmeans_example/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will compile our pipeline defined in the Python script in our local Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py mnist-classification-pipeline.py --output mnist-classification-pipeline.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go back to the instructions and continue with the tasks in the Kubeflow Pipeline Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/f1.png\" alt=\"pipeline\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab #2 - Exploring Amazon SageMaker Components with Elastic Inference and Endpoints with multiple model variants\n",
    "\n",
    "We will now work with a new pipeline for exploring [Amazon Elastic Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html), and [SageMaker Multi-Model Endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html).\n",
    "\n",
    "For this example we will train two models in parallel with different hyper-parameters for illustrating the concept.\n",
    "\n",
    "This will allow us having two different variants of the model in the endpoint, which enables use cases like e.g. A/B testing. You can see more details about this case in the [documentation here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html).\n",
    "\n",
    "We will start by downloading the dataset and preparing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request\n",
    "\n",
    "prefix_name = 'caltech_example'\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "        \n",
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "# caltech-256\n",
    "download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')\n",
    "download('http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two channels: train, validation\n",
    "s3train = 's3://{}/{}/train/'.format(bucket_name, prefix_name)\n",
    "s3validation = 's3://{}/{}/validation/'.format(bucket_name, prefix_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the rec files to train and validation channels\n",
    "!aws s3 cp caltech-256-60-train.rec $s3train\n",
    "!aws s3 cp caltech-256-60-val.rec $s3validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compile our pipeline defined in the Python script in our local Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py caltech-ei-mmv-pipeline.py --output caltech-ei-mmv-pipeline.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go back to the instructions and continue with the tasks in the Kubeflow Pipeline Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/f3.png\" alt=\"pipeline\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab #3 - Exploring Amazon SageMaker Debugger and Model Monitor\n",
    "\n",
    "We will now work with a new pipeline for exploring [SageMaker Debugger](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html), and [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html).\n",
    "\n",
    "For this example we will train a very simple pipeline with the XGBoost algorithm and the same data from the Lab \\#1 above. We will use poor hyper-parameters choices on purpose, which will allow us showing how Debugger highlights the issues in the training.\n",
    "\n",
    "Now, we will compile our pipeline defined in the Python script in our local Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py debugger-monitor-pipeline.py --output debugger-monitor-pipeline.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go back to the instructions and continue with the tasks in the Kubeflow Pipeline Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/f5.png\" alt=\"pipeline\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will enable Model Monitor on the endpoint, so that we can analyze the inputs and outputs to the model inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For doing that go back to your Jupyter environment and open the \"SageMaker Examples\" tab, look for the section \"SageMaker Model Monitor\" and for the notebook called \"SageMaker-Enable-Model-Monitor.ipynb\" click on \"Use\". This will create a copy of this notebook on a folder in your Jupyter environment.\n",
    "\n",
    "Then repeat the steps for the notebook called \"SageMaker-Model-Monitor-Visualize.ipynb\".\n",
    "\n",
    "<img src=\"./images/f6.png\" alt=\"pipeline\" width=\"600\"/>\n",
    "\n",
    "You can now proceed running those notebooks in order (first the enable then the visualize) for completing this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
